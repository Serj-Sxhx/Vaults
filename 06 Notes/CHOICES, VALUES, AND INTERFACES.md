# CHOICES, VALUES, AND INTERFACES

One of the rare upsides of the fractiousness of social media is a growing awareness of the impact of design. 

Human nature hasn't changed over the past two decades, but the [choice architectures](https://en.wikipedia.org/wiki/Choice_architecture) we inhabit have 

[[choice architecture]]
[transformed radically](https://www.pewresearch.org/fact-tank/2019/09/09/us-generations-technology-use/) -- 

with behavioral consequences on full display. The power of design to shape our choices is not a new phenomenon; researchers from architecture to economics have long studied the ways in which affordances and incentives drive our daily decisions. 

But the speed with which we've migrated online has illuminated, for many, the sea of design choices in which we daily swim.

Thankfully, there is a vast literature in economics, philosophy, and social science to guide us as we try to improve the waters. Many modern day algorithms conceive of users as narrow maximizers of whatever metric the service cares to collect (watch time, engagement, time on site, etc.). 

This echoes the early view in economics of humans as rational [utility maximizers](https://en.wikipedia.org/wiki/Homo_economicus) who reveal their preferences through the purchases they make. 

But as many economists and philosophers have discovered, there are problems with this view. 

For example, we often make decisions for [reasons other than pure utility-maximization](https://cdn.uclouvain.be/public/Exports%20reddot/cr-cridis/documents/sen_on_TCR_rational_fools.pdf) (to exercise certain values, for instance), and our [preferences often change](http://bear.warrington.ufl.edu/brenner/mar7588/Papers/slovic-ampsy1995.pdf) across time. 

Moreover, are choices are driven more by the [menu of options](https://www.latimes.com/archives/la-xpm-2008-apr-02-oe-thalerandsunstein2-story.html) available to us at a given time than by any abstract conception of what we value.

A number of philosopher-designers have begun applying these lessons to technological design, among them the former Couchsurfing CTO Joe Edelman. 

Edelman, who coined the term "Time Well Spent" in 2017, has started teaching courses in "[Human Systems](https://www.notion.so/Human-Systems-FAQ-f4f7ad008c5043f6946e43544b328ecd)" to designers around the world, using the insights of economists and philosophers like Amartya Sen and Martha Nussbaum to bring the language of values to technology. 

Edelman [writes](https://medium.com/what-to-build/is-anything-worth-maximizing-d11e648eb56f):

_What we’re missing has different names. Sometimes it is called \[users'\] values, their goals, their identity, or their reasons...to understand someone and to really cooperate with them, you need to know something about \[these things\]. You need to know not just what they do, but why they do it and what they’re hoping for._

Take YouTube as a case in point. We might go to YouTube for many reasons: to learn something new, to feel connected, to be inspired, and so on. But YouTube is blind to these reasons; they are reduced to the narrow metric of watch time, which the service seeks to maximize. That metric in turn feeds back into the menu of options the [recommendation algorithm](https://towardsdatascience.com/how-youtube-recommends-videos-b6e003a5ab2f) offers us, few of which reflect our original reasons for coming to YouTube. This inability for YouTube to _share our reasons_, Edelman argues, creates a breakdown of trust. Our interests and YouTube's interests are fundamentally misaligned.

This lens may help shed light on the [hotly disputed](https://www.bbc.com/ideas/videos/is-technology-addiction-a-myth/p07ggx85) question of "tech addiction." As Jeff Hancock notes in [our interview with him](https://www.youtube.com/watch?v=jzfLoFqVdrY&list=PLhM_FlxNBM3rMZjncOj6TAcv-BthflBau&index=8), using the term "addiction" around technology denies [[agency]] to the user and creates stigma where perhaps none should exist. 

But as Edelman [points out](http://nxhx.org/Choicemaking/), "the economics literature suggests that even the most hardcore addictions can be considered desperate choices" -- the result of either "a bad situation" or "a misunderstanding." 

Seen this way, compulsive email checking or endless Twitter scrolling are not reflections of a lapse in [[agency]] but of a poor choice architecture. 

I'm not told, when logging onto Facebook, about 
- the likely time cost of my actions, 
- or the unlikelihood of fulfilling my initial goal (to rest, to connect, etc.). 

(Improving my menu of options or my knowledge at the moment of choice is, as we noted in [a prior newsletter](http://archive.aweber.com/awlist5048091/LKTkZ/h/Minds_Machines_.htm), what Cass Sunstein [has called](https://poseidon01.ssrn.com/delivery.php?ID=781006013127072084084069104122086109058027047084089074075020083109111106010081064076100027018126119126005096097029004098071076106017028001034081012124071013085011048064000073083125110012122002064087091125072027122069124087127121025106125065098104026&EXT=pdf) "navigability.")

So how might behavioral scientists contribute to this effort? Psychologists already have a [vast storehouse](https://www.amazon.com/dp/B005ERIRZE) of knowledge about when and how we make "desperate choices," as well as the [core values and motivations](https://www.psychologytoday.com/us/blog/in-hindsight/201303/what-moves-us) that drive human behavior. 

This research could help clarify the gaps between the needs we seek to meet with technology and the outcomes these services provide.

But perhaps most important are accounts of how we revise and improve our values across time through reflection and choice-making.

"What is even more important to a person than their current goals and preferences?" Edelman [asks](https://medium.com/what-to-build/whats-next-4b4d00bd9403). 

"The process of refining, discovering, and clarifying those goals and preferences." 

Psychologists can support this process by helping designers understand not just what we do, but what we value and what we might become.